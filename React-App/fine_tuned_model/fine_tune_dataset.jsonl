{"prompt": "What is a (Neural Network) NN?", "completion": "Single neuron == linear regression\nRELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.\nHidden layers predicts connection between inputs automatically, thats what deep learning is good at.\nDeep NN consists of more hidden layers (Deeper layers)\nEach Input will be connected to the hidden layer and the NN will decide the connections.\nSupervised learning means we have the (X,Y) and we need to get the function that maps X to Y."}
{"prompt": "Supervised learning with neural networks", "completion": "Different types of neural networks for supervised learning which includes:\nCNN or convolutional neural networks (Useful in computer vision)\nRNN or Recurrent neural networks (Useful in Speech recognition or NLP)\nStandard NN (Useful for Structured data)\nHybrid/custom NN or a Collection of NNs types"}
{"prompt": "Why is deep learning taking off?\n", "completion": "Deep learning is taking off for 3 reasons:\nData:\nUsing this image we can conclude:\n\nFor small data NN can perform as Linear regression or SVM (Support vector machine)\nFor big data a small NN is better that SVM\nFor big data a big NN is better that a medium NN is better that small NN.\nHopefully we have a lot of data because the world is using the computer a little bit more\nMobiles\nIOT (Internet of things)\nComputation:\nGPUs.\nPowerful CPUs.\nDistributed computing.\nASICs\nAlgorithm:\nCreative algorithms has appeared that changed the way NN works.\nFor example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem."}
{"prompt": "Logistic regression cost function", "completion": "First loss function would be the square root error: L(y',y) = 1/2 (y' - y)^2\nBut we won't use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.\nThis is the function that we will use: L(y',y) = - (y*log(y') + (1-y)*log(1-y'))\nTo explain the last function lets see:\nif y = 1 ==> L(y',1) = -log(y') ==> we want y' to be the largest ==> y' biggest value is 1\nif y = 0 ==> L(y',0) = -log(1-y') ==> we want 1-y' to be the largest ==> y' to be smaller as possible because it can only has 1 value.\nThen the Cost function will be: J(w,b) = (1/m) * Sum(L(y'[i],y[i]))\nThe loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set."}
{"prompt": "Gradient Descent\n", "completion": "We want to predict w and b that minimize the cost function.\n\nOur cost function is convex.\n\nFirst we initialize w and b to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.\n\nIn Logistic regression people always use 0,0 instead of random.\n\nThe gradient decent algorithm repeats: w = w - alpha * dw where alpha is the learning rate and dw is the derivative of w (Change to w) The derivative is also the slope of w\n\nLooks like greedy algorithms. the derivative give us the direction to improve our parameters.\n\nThe actual equations we will implement:\n\nw = w - alpha * d(J(w,b) / dw) (how much the function slopes in the w direction)\nb = b - alpha * d(J(w,b) / db) (how much the function slopes in the d direction)"}
